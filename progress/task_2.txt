Задание (до 21.10.22):
1) разобраться с метриками для аплифта
2) на каггл посмотреть соревнование на аплифт:
    https://www.kaggle.com/datasets/arashnic/uplift-modeling
	прочитать стаатью по данным (просто полезно)
	посмотреть базовое решение и запустить его с данными
	попробовать улучшить базовое решение
	попробовать написать свое решение


Задание на долгий срок:
1) прослушать МО-2 от Соколова



Что сделал (кратко, ниже подробнее)
0) сразу попробовал посмотреть базовое решение для датасета с каггл, 
   написал свое базовое с sklearn RF, 
   понял, что надо читать статьи и чужой код с примерами
1) перечитал туториалы с хабра от МТС
2) пересмотрел доклад по аплифт от МТС
3) пересмотрел 2 доклада по аплифт Валерия Бабушкина
4) прочитал статью Артема Бетлея о DDR/SDR для несбалансированных treatment/control групп
5) прочитал статью Radcliffe 2007 года про использование контрольных групп и метрики Qini (скинуть)
6) прочитал чисто математическую статью Radcliffe & Surry про метрики аплифта с теоремами (скинуть)
7) перечитал статью Radcliffe & Surry (2011) про sig-based trees (скинуть)
8) пересмотрел доклад Максима Коматовского на митапе в августе
9) прочитал ноутбук с каггл с кодом для построения метрик, не уверен
   аплифт считался как Lai’s Generalized Weighed Uplift (LGWUM) (у меня скачан файл как conversion_uplit_e_commerce.pdf) (скинуть)
   https://www.kaggle.com/code/davinwijaya/uplift-modeling-qini-curve-with-python/notebook
10) прочитал 4 ноутбука от команды МТС с примером использования scikit-uplift, sklearn pipeline, пример EDA
11) на примере Титаника и HousePrices потренировался делать EDA, генерить и отбирать признаки, посмотрел чужой код
    (кто-то просто пришел к использованию одной модели из catboost, кто-то обучал целый список моделей и выбирал лучшую через pycaret)


Оставшиеся вопросы
1) binary outcome VS continous outcome in uplift modelling (в статье Radcliffe 2007 вроде бы было про это)
2) почему даже в теории Q = 1 невозможно достичь даже с идеальной информацией (было в статье про метрики)
3) Как проводить отбор признаков и анализ датасета с помощью Net Information Value и Net Weight of Evidence
4) uplift & qini кривые -- в чем разница в scikit-uplift? 
    (было про них в докладе Ирины Елисовой из МТС, что у них просто немного разные формулы)
    посмотрел код на гитхабе scikit-uplift, там формула аплифт кривой имеет вид разности 
        response rates, помноженной на число всех пользователей (вычисляется для каждого k% клиентов от 1 до 100)
		результат -- график с масштабами осей в числах людей (как и qini кривая)
	qini кривая -- разность response rates (в числах людей), но второе число помножено на отношение чисел
		людей в целевой и контрльной группах
5) Victor Lo (2002, пока не читал)




Последовательность вопросов/проблем/удивлений:
1) (удивление) 13 млн строк в данных, базовое решение (hist grad boost) не дало ни одного FP, TP, но дала почти 100% TN
	мысли:  в самом начале даже ноутбука было сказано, что это специфическая задача с сильно несбалансированными классами
		ROC AUC не может учитывать все сразу, она показывает общее качество семейства классификаторов с разными порогами
		это все даже при использовании stratified k fold
2) (удивление) базовая модель дала ROC AUC 0.49-0.50 на 3 фолдах при кросс-валидации (то есть как рандомный ...)
	мысли:  см пункт 1
		стоит посмотреть разные метрики классификации, а так же стоит учитывать особенности аплифта -- нужны другие метрики
		нужно посмотреть не на ROC кривую, а на Lift кривую, разные пакеты (sklift ... от МТС), самому написать
3) (вопрос) какие модели стоит попробовать ?
	мысли:  тут был градиентый бустинг с 4000 деревьев, можно поиграться с их количеством и их глубиной, так как он может сильно
		переобучаться, можно попробовать не бустинг, а бэггинг (случайный лес)
		можно попробовать и более простые модели (линейные, просто для сравнения)
		можно строить ансамбли из разных моделей с учетом весов "правильности" моделей (блендинг)
4) (вопрос) что делать с признаками?
	мысли:  фильтровать, генерировать новые, снова фильтровать (feature importanse: PCA, RandmoForest.featur_importance_, ...)
		полиномиальные фичи для числовых признаков, SVM с полиномиальными ядрами
5) (вопрос) какие методы использования моделей пробовать ?
	мысли:  в статьях на хабре и обзорной (та, что на 30 стр) были описаны методы моделирования аплифта
		две модели, отдельно работающие на тех, кто отреагировал и нет
		одна модель, работающая с флагом воздействия на пользователя
		зависимые/независимые модели (было на хабре)




Процесс выполнения задания:
1) начал перечитывать туториал с хабра (пока только часть 1)
	X-learner (две зависимые модели) в [6] сп. лит. хабра часть 1 советуют применять при малой целевой группе
	общая идея : несбалансированная выборка по treat/control --> скоры от 1-й модели делаем фичами 2-й, у нее больше данных
2) пересмотрел доклад Ирины Елисовой (ссылка в 1 части туториала на хабре)
	там услышал что в статье Артема Бетлея ([5] из сп.лит. 1 части туториала) хорошо описаны метрики
		Uplift Prediction with Dependent Feature Representation in Imbalanced Treatment and Control Conditions
	посмотрел в сп.лит. части 1, увидел статью Radcliffe (2007), про нее видел, что в ней тоже неплохо описаны метрики
		Using Control Groups to Target on Predicted Lift: Building and Assessing Uplift Models
3) прочитал две статьи из прошлого пункта и статью только про метрики (с KDD 2018)
	статья Radcliffe (2007)
		Qini curve предложена как аналог ROC curve, но для изображения работы аплифт модели, для оценки аплифта
		чем она выше, тем лучше, все как у ROC кривой, просто формула отличается, ну и поведение мб разное у нее
		оптимальная Qini curve -- всех откликнувшихся в топ, далее не откликнувшиеся, но и не навредившие, затем навредившие
		навредившие / отказавшиеся сотрудничать -- негативные эффекты воздействия, которые хотелось бы избежать
		если нет негативных эффектов, то Qini curve идет сначала под 45 градусов (по оси Y # чел/, а не доля выборки),
			т.к. сначала все подверженные воздействию, затем нет откликов новых, поэтому аплифт выше не может быть, 
			а затем конец ... в этом случае
		а вот если есть негативные эффекты, то в конце не просто конец, а падение под 45 вних у Qini кривой, так как
			эти люди отказываются от сделки, то есть они в минус уводят инкрементальный эффект, поэтому конечная точка
			находится ниже максимума Qini curve
		если же построить Qini curve так, как будто не было негативных эффектов, то конечная точка у нее будет как и у прежней,
			учитывающей негативные эффекты, но только это точка будем на уровне максимума этой кривой, которая на максимуме
			равна константе, а до этого она тоже идет под 45, поэтому получим оптимальную "zero downlift curve", которая
			выглядит как обычная Qini без негативных эффектов, но заканчивающаяся в точке, которая является конечной для
			Qini curve, учитывающей эти негативные эффекты
		Q = (площадь м/у диагональю и кривой Qini нашей модели) / (площадь м/у диагональю и оптимальной кривой Qini)
		q0 = (площадь м/у диагональю и кривой Qini нашей модели) / (площадь м/у диагональю и "zero downlift curve" optim Qini)
		q0 может быть больше 100%

		ещё -- почему аплифт модели строят для хорошо продаваемых продуктов ?
			потому что если продукт продается не очень хорошо, то те, кто его мог бы купить, под воздействием могут не купить
			поэтому лучше смотреть не аплифт, а просто вероятность покупки при коммуникации, то есть ... response modelling
			но также и потому, что данных просто мало для посроения модели, определяющей РАЗНИЦУ вероятностей покупки при и без воздействия на клиента
			
		обычно причины у доп откликов из-за коммуникации и откликов, которые и так происходят -- разные
			поэтому традиционные модели показывают себя хуже, чем аплифтовые, и иногда хуже случайного коммуницирования...
		вполне обычная картина : usual response rate 1%, after uplift modelling 1.1%
			это, и вычитание моделей со своими ошибками предсказания, может привести к большой ошибке итоговой модели ...
		также стоит обращать внимание на зашумленность данных -- эффект аплифта около 5-20% от обычного числа откликов
			поэтому от шума надо избавляться
	
	советы из Radcliffe(2007)
		variable selection
		binning methods
		bagging
		noise reduction
		stratified sampling
		cross validation
	
	статья Артема Бетлея ([5] из сп.лит. 1 части туториала)
		там предлагаются два пути решения проблемы несбалансированности целевой и контрольной групп
			DDR (Dependent Data Representation) -- предсказания одной модели становятся фичами другой, которая выучит аплифт
				как метод двух моделей с хабра, но только тут ко второй модели добавляем фичи
			SDR (SharedData Representation) -- есть признаки, есть индикатор целевой группы, делаем фичами их произведения
				можно отдельно регуляризовать веса изначальных фичей w_0 и веса признаков произведений индикатора и фичей w_1
		описаны разные уже существующие методы моделирования аплифт
			две независмые модели
			преобразование таргета (P_T(Y=1|X) - P_C(Y=1|X) = 2P(Z=1|X) - 1, X = YT + (1-Y)(1-T)) -- и дальше любой классификатор
				недостаток: Z=1 объединяет две разные группы с одним исходом, в случае дисбаланся T,C ... сложно
			есть ссылки на статьи об SVM (там нужны 2 гиперплоскости уже ...) и деревьях в аплифте
		также там рассказывается о метриках с единой точки зрения и том, как выбрать нужную метрику в конкретной задаче
			в случае дисбаланса Treatment, Control групп стоит использовать кривую Qini, она использует нормировку на размеры классов
			если со временем новые данные изменят баланс классов -- то метрика Qini сильно не потсрадает, она робастна к такому
		их эксперименты были на двух датасетах
			Hillstrom dataset == results of an e-mail campaign for an Internet based retailer
			CRITEO-UPLIFT1 == incrementality test, random part of the population not targeted by ads
			предобрабатывают, используют sklearn, делают 50 рандомных сплитов на трейн/тест с T/C как 70/30 (Treatment / Control)
			DDR, SDR -- для них делают регуляризацию отдельно на признаках и их произведениях с индикатором воздействия, подбор по сетке
			проверяют статистически значимое различие парным t-test 5%
			результаты работы метода DDR
				DDR -- улучшение two-model подхода, поэтому с ним и сравнивали, на Hillstrom dataset
				контроль брали 100%, 50%, 10% от изначального объема, имтировали несбалансированность Control Treatment
				DDR лучше two-model на несбалансированных T-C выборках в терминах метрики Q
				сравнили C->T схему обучения моделей (одну на Control, ее предсказания фичами для модели на Treatment) и T->C
					C->T показала себя лучше для несбалансированных выборок
					C->T -- uplift = P_T(Y|X,P_C(X)) - P_C(X)
				проверили на сложность взаимосвязь treatment и фичей -- можно ли предсказывать просто константу на группе T или C
					оказалось, что нельзя, есть некоторые взаимосвязи, причем если predict const для T, то результат ещё хуже
			результаты работы метода SDR
				сравнивали с методом трансформации таргета (Revert Label, формула выше была с Z = YT + (1-Y)(1-T))
				SDR куда лучше на несбалансированных выборках, на сбалансированной он тоже лучше, чем Revert Label
				провели эксперимент по оценке важности фичей, являющихся произведениями изначальных фичей и индикатора Treatment
				получилось, что они очень важны, без них качество модели заметно ниже
			в планах указали исследование сильно нелинейных зависимостей для поиска взаимосвязи между treatment индикатором и откликом
4) посмотрел два выступления Валерия Бабушкина по аплифту, там была ссылка на статью 2013 года (Гельман ... ?), где был предложен
    метод построения деревьев с заменой impurity (H(X) -- информационный критерий) на аплифт, который будет максимизироваться (разница
    между нодой и суммой ее двух потомков -> max), но есть реализация только на R, на Python нет, поэтому надо будет написать самому, но
    формулы есть в его слайдах
5) прочитал ноутбук на каггл (https://www.kaggle.com/code/davinwijaya/uplift-modeling-qini-curve-with-python/notebook)
    там посмотрел пример построения кривой Qini, анализа данных и работы с модельками, типы моделек  ("для ознакомления")
	довольно хорошо показывает как можно закодить метрики и как вывести графики кривых Qini
	простой OHE, df на два df с разными treatment ('discount', 'buy one get one'), 4 класса клиентов по реакции на воздействие с LabelEncoding
	метод от Victor Lo (2002, пока не читал)
6) прочитал туториал на хабре часть 2, проявилось непонимание, как именно вычислять KL(P:Q) для разбиения ноды на два листа в аплифт дереве
    там смотрится KL(P_L^T, P_L^C) + KL(P_R^T, P_R^C) - KL(P^T, P^C) ... ? L-left, R-right, без нижнего индекса -- в текущей ноде
    при этом просто смотрят на доли откликнувшихся людей их контрольной и целевой групп, и хотят через KL получить максимальное расхождение 
    в этих долях по двум новым нодам относительно исходной ноды ... ? 
7) прочитал туториал на хабре часть 3 про метрики, вроде бы вопросов не возникло, подумал использовать уже их библиотеку, 
    но может только дерево написать свое, потренироваться, да и у них в моделях не увидел модели такой ... 
8) пересмотрел по диагонали с остановками на графиках, таблицах, формулах статью Radcliffe & Surry (2011) "Real world uplift modelling: significance-based uplift trees"
	significanse-based trees:  
		significanse-based критерий разбиения
			обычные деревья стремятся
				разница в outcomes для поддеревьев --> max
				разница в размераз для поддеревьев --> min
			предлагаемый критерий смотрит не на purchase rate или что-то похожее, а на общий аплифт
			критерий "delta delta p" (просто разница в аплифтах поддеревьев) тут не помог, но Hansotia & Rukstales (Incremental value modeling, 2001) помог
			критерий просто в виде Qini метрики ухудшил качество (возможно, т.к. показывает лишь качество ранжирования объектов)
			ad hoc критерий с uplift penalizing (delta / ((N_L + N_R) / 2min(N_L, N_R))^k), учитывающим размеры поддеревьев
				но подбрать оптимальное k у Radcliffe & Surry для реальных задач не получилось
			метод из статьи строит для каждого возможного сплита линейную модель и смотрит на важность interaction term как на критерий качества разбиения
				важность -- t-statistic (Jennings, Statistics 512: Applied linear models, topic 3, chapter 5. Technical report, Perdue University,2004)
				используют оценку для вычисления важности gamma_TR
		variance-based pruning
			обычно строят деревья полностью, а затем их стригут, потому что качество разбиения в вершине иногда становится понятно гораздо глубже в дереве
			также обычно вносят 2 ограничения в построение дерева: max_depth, min_samples_split
			могут рассматривать не все сплиты при поиске оптимального, для ускорения
			в аплифте есть три проблемы:
				достигаемый аплифт зачастую заметно меньше основного эффекта (background effect)
				размер контроля зачастую меньше тритмента в 10 и более раз (иначе невыгодно)
				аплифт -- феномен второго порядка, поэтому ошибки из-за оценивания аплифта могут быть довольно большими
			значит, модели аплифта должны быть крайне стабильными
			они делают k~8 выборок из исходной тренировочной с возвращением (bootstrap), на одной тренеруют, на остальных оценивают
				на 1 в каждом узле дерева находят аплифт, на остальных находят стандартное отклонение аплифта в этом узле
				если стандартное отклонение аплифта в этом узле больше некоторого порога, то такой узел (и все его потомки) отбрасывается
				но все это сильно зависит от датасета и сложно обобщить
			валидационную выборку не трогают при таком подходе
		bagging
			10-20 раз обучают модели
			для каждой модели делают 8 ресемплингов изначальной выборки
			ещё используют отбор признаков, поэтому частично метод похож на случайный лес
		pessimistic qini-based variable selection
			по мнению авторов он ещё более важен в аплифт моделировании, чем в традиционном моделировании (где есть истинные ответы)
			в традиционном моделировании отбор признаков мотивируется:
				уменьшением размерности признакового пространства и вероятности переобучения
				уменьшением корреляции признаков
					сильно скоррелированные признаки вызывают увеличение веса одного и уменьшение веса другого, что плохо интерпретируется, и числовые неточности
				улучшение качества и стабильности модели
					оставим плохой признак -- модель "деградирует" (лишняя хромосома ...)
					удалим позже где-то -- модель получит субоптимальный набор из других фичей, но не оптимальный
				улучшает интерпретируемость модели
					разные переменные --> разные интерпретации
					коррелирующие переменные ухудшают интерпретируемость
			а в аплифт моделировании отбор признаков мотивируется:
				теми же причинами, что и в традиционном моделировании
				но третий пункт (качество, стабильность модели) выходит на первый план
				ведь аплифт модели находят оценку "difference between two outcomes, not direct outcome", а также аплифт довольно мал по сравнению с "outcomes"
				у авторов статьи без отбора признаков все модели падали на валидации, ни каких практикозначимых результатов без отбора признаков получить не удалось
			желаемые свойства метода отбора признаков:
				предсказательная сила (predictiveness) -- первый приоритет, иначе любая модель будет бесполезна
				устойчивость (robustness) -- для аплифт моделей ещё куда важнее, чем для обычных моделей
				независимость (independence) -- хочется, чтобы признаки улавливали разные зависимости, чтобы их набор был "минимальным по включению"
			Pessimistic Qini Estimates
				quality measure --> rank candidate variables --> choose best ones
				делают бутстрапом n~8 выборок, оценивают на них qini
				из оценки итоговой вычитают s (sample starndard deviation ...че это ...) * coeff, coeff \in (0, 1) (но чаще от 0.5 до 1)
				больше coeff -- больше стабильность, меньше coeff -- больше предсказательная способность
			Net Information Value
				ещё один способ отбора признаков (Larsen, 2010) для аплифт моделей с бинарным выходом
				авторы не пробовали этот подход
9) пересмотрел доклад Максима Коматовского на митапе в августе, все понял кроме бизнесовой части (которой было немного)
	(несколько контрольных групп, методы коммуникации, правила понижения и повышения ставок)
10) прочитал статью Radcliffe, Surry про метрики с KDD 2011
	придумали метрику по аналогии с Gini, но для аплифта, построили графики, доказали некоторые утверждения про метрики для более удобного их вычисления
	показали, что один из них может служить удобной интерпретацией статистики Колмогорова-Смирнова (KS = max F_+(x) - F_-(x)) в аплифт моделировании,
		которая в задаче обычной бинарной классификации показывает качество класиификации (больше -- лучше, распределения 
		сильнее рознятся, то есть и разделяются объекты лучше)
11) написал код для датасета Criteo (только RF), Qini with neg effects ~ 0.14 (но по идее меньше может быть, надо проверить код)
	узнал о библиотеках pylift, scikit-uplift, causalml
	для Титаника и HousePrices на каггл посмотрел обработку данных, генерацию и отбор признаков, выбор модели
    узнал о библиотеках pycaret, mlextend
12) прочитал ноутбук с сайта scikit-uplift по EDA для criteo, понял, что из колонок метки и флаги воздействия
    прочитал вводную страничку с этого же сайта
	прочитал ноутбуки с гитхаба scikit-uplift про pipeline, metrics, metrics advanced
		по пути залез в имплементацию кривых uplift, qini (оси имеют масштаб в числах людей)




О датасете uplift criteo:
1) из ноутбука с EDA на сайте scikit-uplift узнал, что в данных есть признаки взаимодействия/эффекта, а что метки.
   treatment и exposure -- один блок признаков (воздействия)
   visit и conversion -- блок меток 
   это сделано для возможности моделировать разные исходы при разных признаках (
   default parameters: target = 'visit', treatment = 'treatment'



Словарик
 0) net effect -- чистый эффект
 1) attrition -- отсев, отток, выбывание ... attrition modelling -- моделирование оттока
 2) utility -- полезность
 3) former case -- предыдущий случай
 4) excess -- избыток, излишек, превышение
 5) invoke -- сослаться, вызвать (invoke negative effects -- "не учитывая/рассматривая/помня негативные эффекты" ... ? )
 6) problem was tackled -- ... решена 
 7) models diverge -- расходятся (devergence -- дивергенция, расхождение) 
 8) witness -- свидетель, очевидец
    witness smth. -- быть свидетелем
 9) The results are striking -- ... поразительные
10) to tackle -- бороться с, решать (проблему), "для решения (этой проблемы)"
11) to be doom to failure -- быть обреченным на провал, неудачу, поражение
12) substantial -- существенный
13) interaction, intervention -- взаимодействие, вмешательство
14) merits -- достоинства, заслуги, преимущества
15) in the former -- в прежнем, в первом (например, случае)
    in the latter -- в последнем (например, случае)
		it is studied in ... and ...
		in the FORMER (the first ...), it is ...
		in the LATTER (the second ...), it is ...
16) treatment is exposure to medicine -- лечение заключается в воздействии препаратов
    exposing users to ads -- демонстрация пользователям рекламы
17) enforce -- соблюдать
18) to inform further actions -- для обоснвания дальнейших действий
19) Our main contributions are threefold. -- Наш основной вклад состоит из трех частей.
20) a unified view -- единый взгляд
21) palpable evidence -- ощутимое (весомое) доказательство
22) to posit -- утверждать, позиционировать
23) drastically -- кардинально
24) conjunction -- связь, объединение
25) covariate -- независимая переменная (одно из значений)
26) proceed -- продолжить, перейти к
27) drawbacks -- недостатки
28) investigate -- изучить, исследовать
29) conversely -- напротив, наоборот
30) plausible -- правдоподобный
31) whereby -- посредством чего
32) it comes to the fore -- выходит на первый план
33) 